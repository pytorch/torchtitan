{
  "added_tokens_decoder": {
    "128000": {
      "content": "<|begin_of_text|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "128001": {
      "content": "<|end_of_text|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "1998": {
      "content": "<|image|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "1999": {
      "content": "<|begin_of_image|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "2000": {
      "content": "<|end_of_image|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    },
    "2001": {
      "content": "<|pad|>",
      "lstrip": false,
      "normalized": false,
      "rstrip": false,
      "single_word": false,
      "special": true
    }
  },
  "bos_token": "<|begin_of_text|>",
  "clean_up_tokenization_spaces": true,
  "eos_token": "<|end_of_text|>",
  "img_token": "<|image|>",
  "boi_token": "<|begin_of_image|>",
  "eoi_token": "<|end_of_image|>",
  "pad_token": "<|pad|>",
  "model_input_names": [
    "input_ids",
    "attention_mask"
  ],
  "model_max_length": 131072,
  "tokenizer_class": "PreTrainedTokenizerFast"
}
