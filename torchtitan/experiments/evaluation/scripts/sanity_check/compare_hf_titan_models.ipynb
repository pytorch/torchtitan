{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the BSD-style license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "# Copyright (c) Meta Platforms, Inc. All Rights Reserved.\n",
    "\n",
    "PROJECT_ROOT = \"\"  # TODO: Change to the correct path if needed\n",
    "\n",
    "SAVE_ROOT = \"\"  # TODO: Change to the correct path if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e078f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangminbae/miniconda3/envs/titan/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(PROJECT_ROOT)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tyro\n",
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "import torchtitan.protocols.train_spec as train_spec_module\n",
    "from torchtitan.components.checkpoint import CheckpointManager\n",
    "from torchtitan.config import ConfigManager\n",
    "from torchtitan.tools import utils\n",
    "from torchtitan.experiments.evaluation.generator.utils import DummyOptimizerContainer, DummyLRSchedulerContainer\n",
    "\n",
    "TOML = {\n",
    "    \"llama3_2_1b\": \"torchtitan/experiments/evaluation/llama3/train_configs/llama3.2_1b.toml\",\n",
    "    \"llama3_2_3b\": \"torchtitan/experiments/evaluation/llama3/train_configs/llama3.2_3b.toml\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78642c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name =  \"llama_3.2_1b_dcp\"  # TODO: Change to the correct experiment name if needed\n",
    "model_size = \"llama3_2_1b\"\n",
    "\n",
    "file_path = os.path.join(PROJECT_ROOT, TOML[model_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3de873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ConfigManager and load the configuration\n",
    "config_manager = ConfigManager()\n",
    "\n",
    "args = [f\"--job.config_file={file_path}\",]\n",
    "\n",
    "toml_values = config_manager._maybe_load_toml(args)\n",
    "config_cls = config_manager._maybe_add_custom_args(args, toml_values)\n",
    "\n",
    "base_config = (\n",
    "    config_manager._dict_to_dataclass(config_cls, toml_values)\n",
    "    if toml_values\n",
    "    else config_cls()\n",
    ")\n",
    "custom_registry = tyro.constructors.ConstructorRegistry()\n",
    "job_config = tyro.cli(\n",
    "    config_cls, args=args, default=base_config, registry=custom_registry\n",
    ")\n",
    "\n",
    "# TODO: Change the dataset and dataset path if needed\n",
    "job_config.training.dataset = \"c4_test\"\n",
    "job_config.training.dataset_path = \"/home/sangminbae/torchtitan/tests/assets/c4_test\"\n",
    "\n",
    "job_config.job.dump_folder = os.path.join(SAVE_ROOT, \"outputs\", exp_name)\n",
    "job_config.checkpoint.folder = os.path.join(SAVE_ROOT, \"checkpoints\", exp_name)\n",
    "job_config.checkpoint.load_step = -1\n",
    "job_config.checkpoint.enable_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fd3671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sangminbae/miniconda3/envs/titan/lib/python3.12/site-packages/torch/distributed/checkpoint/state_dict_loader.py:153: UserWarning: torch.distributed is disabled, unavailable or uninitialized, assuming the intent is to load in a single process.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Build Model and Tokenizer\n",
    "train_spec = train_spec_module.get_train_spec(job_config.model.name)\n",
    "\n",
    "# build model (using meta init)\n",
    "model_cls = train_spec.model_cls\n",
    "model_args = train_spec.model_args[job_config.model.flavor]\n",
    "\n",
    "tokenizer = (\n",
    "    train_spec.build_tokenizer_fn(job_config)\n",
    "    if train_spec.build_tokenizer_fn is not None\n",
    "    else None\n",
    ")\n",
    "# set the model args from training job configs\n",
    "model_args.update_from_config(job_config)\n",
    "\n",
    "with torch.device(\"cuda\"):\n",
    "    model = model_cls(model_args)\n",
    "\n",
    "dummy_dataloader = train_spec.build_dataloader_fn(\n",
    "    dp_world_size=1,\n",
    "    dp_rank=0,\n",
    "    tokenizer=tokenizer,\n",
    "    job_config=job_config,\n",
    ")\n",
    "\n",
    "dummy_optimizers = DummyOptimizerContainer()\n",
    "dummy_lr_schedulers = DummyLRSchedulerContainer()\n",
    "\n",
    "checkpointer = CheckpointManager(\n",
    "    dataloader=dummy_dataloader, # from your notebook\n",
    "    model_parts=[model],\n",
    "    optimizers=dummy_optimizers, # Replace with actual or proper dummy\n",
    "    lr_schedulers=dummy_lr_schedulers, # Replace with actual or proper dummy\n",
    "    states={}, # Any other custom states\n",
    "    checkpoint_config=job_config.checkpoint,\n",
    "    sd_adapter=None,\n",
    "    ft_manager=None,\n",
    ")\n",
    "\n",
    "try:\n",
    "    checkpointer.load(step=job_config.checkpoint.load_step)\n",
    "except Exception as e:\n",
    "    # If WORLD_SIZE is different, it might raise an error,\n",
    "    # but checkpoint loading should still work\n",
    "    print(f\"Failed to load checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f93f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from Hugging Face\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bffa40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = train_spec.build_dataloader_fn(\n",
    "    dp_world_size=1,\n",
    "    dp_rank=0,\n",
    "    tokenizer=tokenizer,\n",
    "    job_config=job_config,\n",
    ")\n",
    "\n",
    "data_iterator = iter(dataloader)\n",
    "\n",
    "def next_batch(\n",
    "    data_iterator: Iterable\n",
    ") -> tuple[dict[str, torch.Tensor], torch.Tensor]:\n",
    "    batch = next(data_iterator)\n",
    "    input_dict, labels = batch\n",
    "\n",
    "    device_type = utils.device_type\n",
    "    for k, _ in input_dict.items():\n",
    "        input_dict[k] = input_dict[k].to(device_type)\n",
    "    labels = labels.to(device_type)\n",
    "    return input_dict, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ca1c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchTitan loss: 3.0213506222\n",
      "HuggingFace loss: 3.0213508606\n",
      "TorchTitan loss: 3.1651334763\n",
      "HuggingFace loss: 3.1651337147\n",
      "TorchTitan loss: 3.0910332203\n",
      "HuggingFace loss: 3.0910334587\n",
      "TorchTitan loss: 2.9420754910\n",
      "HuggingFace loss: 2.9420757294\n",
      "TorchTitan loss: 3.0103466511\n",
      "HuggingFace loss: 3.0103468895\n"
     ]
    }
   ],
   "source": [
    "loss_fn = train_spec.build_loss_fn(job_config)\n",
    "\n",
    "sample_numbers = 5\n",
    "\n",
    "model.eval()\n",
    "hf_model.eval()\n",
    "with torch.no_grad():\n",
    "    model.to(\"cuda\")\n",
    "    hf_model.to(\"cuda\")\n",
    "\n",
    "    for idx in range(sample_numbers):\n",
    "        input_dict, labels = next_batch(data_iterator)\n",
    "        input = input_dict[\"input\"]\n",
    "        \n",
    "        pred1 = model(input)\n",
    "        pred2 = hf_model(input)\n",
    "\n",
    "        loss1 = loss_fn(pred1, labels)\n",
    "        print(f\"TorchTitan loss: {loss1.item():.10f}\")\n",
    "        loss2 = loss_fn(pred2.logits, labels)\n",
    "        print(f\"HuggingFace loss: {loss2.item():.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1ee30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "titan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
