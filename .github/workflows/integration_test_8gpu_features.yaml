name: 8 GPU Feature Tests

on:
  push:
    branches: [ main ]
    paths-ignore:
      - 'torchtitan/experiments/**'
  pull_request:
    paths-ignore:
      - 'torchtitan/experiments/**'
  schedule:
    # Runs every 6 hours
    - cron: '0 */6 * * *'

concurrency:
  group: unit-test${{ github.workflow }}-${{ github.ref == 'refs/heads/main' && github.run_number || github.ref }}
  cancel-in-progress: true

defaults:
  run:
    shell: bash -l -eo pipefail {0}

permissions:
      id-token: write
      contents: read

jobs:
  check-disk:
    runs-on: linux.rocm.gpu.gfx942.4
    steps:
      - name: Check disk usage
        run: |
          echo "Checking disk space on ROCm runner before container is created..."
          df -h
          echo "Finished checking disk space."

  build-test:
    needs: check-disk
    # TESTING: TO BE CHANGED BACK TO MAIN ONCE https://github.com/pytorch/test-infra/pull/7379 LANDS
    uses: pytorch/test-infra/.github/workflows/linux_job_v2.yml@expose_runner_temp
    strategy:
      matrix:
        include:
          - name: cuda
            runner: linux.g5.48xlarge.nvidia.gpu
            gpu-arch-type: cuda
            gpu-arch-version: "12.6"
            # This image is faster to clone than the default, but it lacks CC needed by triton
            # (1m25s vs 2m37s).
            docker-image: torchtitan-ubuntu-20.04-clang12
            index-url: https://download.pytorch.org/whl/nightly/cu126
          - name: rocm
            runner: linux.rocm.gpu.gfx942.4
            gpu-arch-type: rocm
            gpu-arch-version: "7.0"
            docker-image: torchtitan-rocm-ubuntu-22.04-clang12
            index-url: https://download.pytorch.org/whl/nightly/rocm7.0
    with:
      # TESTING: TO BE REMOVED ONCE https://github.com/pytorch/test-infra/pull/7379 LANDS
      test-infra-ref: expose_runner_temp
      runner: ${{ matrix.runner }}
      gpu-arch-type: ${{ matrix.gpu-arch-type }}
      gpu-arch-version: ${{ matrix.gpu-arch-version }}
      docker-image: ${{ matrix.docker-image }}
      repository: pytorch/torchtitan
      upload-artifact: outputs
      timeout: 45
      script: |
        set -eux

        # The generic Linux job chooses to use base env, not the one setup by the image
        CONDA_ENV=$(conda env list --json | jq -r ".envs | .[-1]")
        conda activate "${CONDA_ENV}"

        # Log CUDA driver version for debugging.
        DRIVER_VERSION=$(nvidia-smi --query-gpu=driver_version --format=csv,noheader | head -n 1 || true)
        echo "CUDA driver version: ${DRIVER_VERSION}"

        pip config --user set global.progress_bar off

        python -m pip install --force-reinstall --pre torch --index-url ${{ matrix.index-url }}

        USE_CPP=0 python -m pip install --pre torchao --index-url ${{ matrix.index-url }}

        # debugging for CPU shared memory
        # Show total and available disk space on the shared memory filesystem (/dev/shm)
        echo "Running shared memory debug for ROCm runner..."
        df -h /dev/shm
        # Display mount information related to /dev/shm to verify the size and options
        mount | grep /dev/shm
        # List all current shared memory segments in the system
        ipcs -m
        # Display shared memory statistics from /proc/meminfo (total, free, used)
        grep Shmem /proc/meminfo
        echo "Finished debugging..."

        sudo mkdir -p "$RUNNER_TEMP/artifacts-to-be-uploaded"
        sudo chown -R $(id -u):$(id -g) "$RUNNER_TEMP/artifacts-to-be-uploaded"

        export TEST_WITH_ROCM=$([[ "${{ matrix.gpu-arch-type }}" == "rocm" ]] && echo 1 || echo 0)
        python -m tests.integration_tests.run_tests --test_suite features $RUNNER_TEMP/artifacts-to-be-uploaded --ngpu 4
